{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "import csv\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Access data\n",
    "def get_data(data_dir, labels_path):\n",
    "    data_filenames = glob.glob(os.path.join(data_dir, '*npy'))\n",
    "    random.shuffle(data_filenames)\n",
    "\n",
    "    num_examples = len(data_filenames)\n",
    "    \n",
    "    labels_dict = {}\n",
    "    get_labels_dict(labels_path, labels_dict)\n",
    "    \n",
    "    X = np.zeros([num_examples, 151, 100])\n",
    "    Y = np.zeros(num_examples)\n",
    "    for i, df in enumerate(data_filenames):\n",
    "        X[i, :, :] = np.load(df)\n",
    "        \n",
    "        label_key = df.split('/')[-1].split('.')[0].split('_')[0]\n",
    "        Y[i] = labels_dict[label_key]\n",
    "        \n",
    "    # Convert -1 labels to 0\n",
    "    Y[np.where(Y == -1)] = 0\n",
    "        \n",
    "    return X.astype(np.float32), Y.astype(np.float32)\n",
    "\n",
    "\n",
    "def get_labels_dict(reference_path, reference):\n",
    "    with open(reference_path) as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "        for row in spamreader:\n",
    "            reference[row[0]] = row[1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions for defining variables in the network graph\n",
    "def _variable_on_cpu(name, shape, initializer):\n",
    "    \"\"\"\n",
    "    Helper to create a Variable stored on CPU memory.\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "        var = tf.get_variable(\n",
    "            name=name, \n",
    "            shape=shape, \n",
    "            dtype=tf.float32, \n",
    "            initializer=initializer)\n",
    "\n",
    "    return var\n",
    "\n",
    "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "    \"\"\"\n",
    "    Helper to create an initialized Variable with weight decay.\n",
    "    \"\"\"\n",
    "\n",
    "    var = _variable_on_cpu(\n",
    "        name=name,\n",
    "        shape=shape,\n",
    "        initializer=tf.truncated_normal_initializer(\n",
    "            stddev=stddev,\n",
    "            dtype=tf.float32))\n",
    "\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, \n",
    "                                   name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)\n",
    "\n",
    "    return var\n",
    "\n",
    "\n",
    "def loss(unscale_logits, labels):\n",
    "    \"\"\"\n",
    "    Add L2Loss to all the trainable variables.\n",
    "    \"\"\"\n",
    "\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=labels,\n",
    "        logits=unscale_logits,\n",
    "        name='cross_entropy')\n",
    "\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "\n",
    "    tf.add_to_collection('losses', cross_entropy_mean)\n",
    "\n",
    "    total_loss = tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function for defining CNN network\n",
    "def cnn(features):\n",
    "\n",
    "    # Input Layer.\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 151, 100, 1])\n",
    "\n",
    "    # Convolutional layer #1.\n",
    "    with tf.variable_scope(name_or_scope='conv1') as scope:\n",
    "        kernel = _variable_with_weight_decay(\n",
    "            name='weights',\n",
    "            shape=[5, 5, 1, 64],\n",
    "            stddev=5e-2,\n",
    "            wd=0.0)\n",
    "\n",
    "        conv = tf.nn.conv2d(\n",
    "            input=input_layer,\n",
    "            filter=kernel,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding='SAME')\n",
    "\n",
    "        biases = _variable_on_cpu(\n",
    "            name='biases',\n",
    "            shape=[64],\n",
    "            initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "\n",
    "        conv1 = tf.nn.tanh(\n",
    "            x=pre_activation, \n",
    "            name=scope.name)\n",
    "        \n",
    "        \n",
    "    # Max pooling layer #1.\n",
    "    pool1 = tf.nn.max_pool(\n",
    "        value=conv1,\n",
    "        ksize=[1, 3, 3, 1], \n",
    "        strides=[1, 2, 2, 1],\n",
    "        padding='SAME', \n",
    "        name='pool1')\n",
    "\n",
    "    \n",
    "    # Local response normalization layer #1.\n",
    "    norm1 = tf.nn.lrn(\n",
    "        input=pool1, \n",
    "        depth_radius=4, \n",
    "        bias=1.0, \n",
    "        alpha=0.001 / 9.0, \n",
    "        beta=0.75,\n",
    "        name='norm1')\n",
    "    \n",
    "    # Convolutional layer #2.\n",
    "    with tf.variable_scope(name_or_scope='conv2') as scope:\n",
    "        kernel = _variable_with_weight_decay(\n",
    "            name='weights',\n",
    "            shape=[5, 5, 64, 64],\n",
    "            stddev=5e-2,\n",
    "            wd=0.0)\n",
    "\n",
    "        conv = tf.nn.conv2d(\n",
    "            input=norm1,\n",
    "            filter=kernel,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding='SAME')\n",
    "\n",
    "        biases = _variable_on_cpu(\n",
    "            name='biases',\n",
    "            shape=[64],\n",
    "            initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "\n",
    "        conv2 = tf.nn.tanh(\n",
    "            x=pre_activation, \n",
    "            name=scope.name)\n",
    "\n",
    "    # Local response normalization layer #2.\n",
    "    norm2 = tf.nn.lrn(\n",
    "        input=conv2, \n",
    "        depth_radius=4, \n",
    "        bias=1.0, \n",
    "        alpha=0.001 / 9.0, \n",
    "        beta=0.75,\n",
    "        name='norm2')\n",
    "\n",
    "    # Max pooling layer #2.\n",
    "    pool2 = tf.nn.max_pool(\n",
    "        value=norm2,\n",
    "        ksize=[1, 3, 3, 1], \n",
    "        strides=[1, 2, 2, 1],\n",
    "        padding='SAME', \n",
    "        name='pool2')\n",
    "\n",
    "    # Fully connected layer #1.\n",
    "    with tf.variable_scope(name_or_scope='fc1') as scope:\n",
    "        flat = tf.reshape(pool2, [-1, 60800])\n",
    "\n",
    "        weights = _variable_with_weight_decay(\n",
    "            name='weights',\n",
    "            shape=[60800, 384],\n",
    "            stddev=0.04,\n",
    "            wd=0.004)\n",
    "\n",
    "        biases = _variable_on_cpu(\n",
    "            name='biases',\n",
    "            shape=[384],\n",
    "            initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "        pre_activation = tf.add(tf.matmul(flat, weights), biases)\n",
    "\n",
    "        fc1 = tf.nn.relu(\n",
    "            features=pre_activation,\n",
    "            name=scope.name)\n",
    "\n",
    "        \n",
    "    # Fully connected layer #2.\n",
    "    with tf.variable_scope(name_or_scope='fc2') as scope:\n",
    "        weights = _variable_with_weight_decay(\n",
    "            name='weights',\n",
    "            shape=[384, 192],\n",
    "            stddev=0.04,\n",
    "            wd=0.004)\n",
    "\n",
    "        biases = _variable_on_cpu(\n",
    "            name='biases',\n",
    "            shape=[192],\n",
    "            initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "        pre_activation = tf.add(tf.matmul(fc1, weights), biases)\n",
    "\n",
    "        fc2 = tf.nn.relu(\n",
    "            features=pre_activation,\n",
    "            name=scope.name)\n",
    "\n",
    "        \n",
    "    # Unscaled logits layer #1.\n",
    "    with tf.variable_scope(name_or_scope='logits1') as scope:\n",
    "        weights = _variable_with_weight_decay(\n",
    "            name='weights',\n",
    "            shape=[192, 10],\n",
    "            stddev=1.0 / 192.0,\n",
    "            wd=0.0)\n",
    "\n",
    "        biases = _variable_on_cpu(\n",
    "            name='biases',\n",
    "            shape=[10],\n",
    "            initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "        unscale_logits = tf.add(tf.matmul(fc2, weights), biases)\n",
    "        \n",
    "    return unscale_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function for building CNN network\n",
    "def cnn_model_fn(features, labels, mode):\n",
    "    \"\"\"\n",
    "    Build model.\n",
    "    \"\"\"\n",
    "    unscale_logits = cnn(features)\n",
    "        \n",
    "    # Generate predictions for PREDICT and EVAL modes.\n",
    "    predictions = {\n",
    "        'classes': tf.argmax(input=unscale_logits, axis=1),\n",
    "        'probabilities': tf.nn.softmax(unscale_logits, name='softmax_tensor')\n",
    "    }\n",
    "    \n",
    "    ####################\n",
    "    # PREDICT\n",
    "    ####################\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT: \n",
    "        return tf.estimator.EstimatorSpec(mode=mode, \n",
    "                                          predictions=predictions['probabilities'])\n",
    "    \n",
    "    else:\n",
    "        # Calculate loss for both TRAIN and EVAL modes.\n",
    "        labels = tf.cast(labels, tf.int64)\n",
    "        total_loss = loss(unscale_logits, labels)\n",
    "\n",
    "        # Add summary operation for total loss visualizaiton.\n",
    "        tf.summary.scalar(\n",
    "            name='total_loss',\n",
    "            tensor=total_loss)\n",
    "    \n",
    "\n",
    "        ####################\n",
    "        # TRAIN\n",
    "        ####################\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "\n",
    "            # Compute gradients using Gradient Descent Optimizer.\n",
    "            optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "            grads_vars = optimizer.compute_gradients(loss=total_loss)\n",
    "\n",
    "            # Add summary operations for gradient visualizations.\n",
    "            for grad, var in grads_vars:\n",
    "                if grad is not None:\n",
    "                    tf.summary.histogram(\n",
    "                        name=var.op.name + '/gradients', \n",
    "                        values=grad)\n",
    "\n",
    "            train_op = optimizer.minimize(\n",
    "                loss=total_loss,\n",
    "                global_step=tf.train.get_global_step())\n",
    "\n",
    "            # Add evaluation metrics for TRAIN mode.\n",
    "            accuracy_train = tf.metrics.accuracy(\n",
    "                labels=labels, \n",
    "                predictions=predictions[\"classes\"])\n",
    "\n",
    "            # Add summary operation for training accuracy visualizaiton.\n",
    "            tf.summary.scalar(\n",
    "                name='accuracy_train',\n",
    "                tensor=accuracy_train[0])\n",
    "\n",
    "            train_summary_hook = tf.train.SummarySaverHook(\n",
    "                save_steps=10,\n",
    "                output_dir='models/q2-train',\n",
    "                summary_op=tf.summary.merge_all())\n",
    "\n",
    "            return tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=total_loss, \n",
    "                train_op=train_op,\n",
    "                training_hooks=[train_summary_hook])\n",
    "\n",
    "        \n",
    "        ####################\n",
    "        # EVALUATE\n",
    "        ####################\n",
    "        else:\n",
    "            accuracy_valid = tf.metrics.accuracy(\n",
    "                labels=labels, \n",
    "                predictions=predictions[\"classes\"])\n",
    "\n",
    "            # Add summary operation for validation accuracy visualizaiton.\n",
    "            tf.summary.scalar(\n",
    "                name='accuracy_validation',\n",
    "                tensor=accuracy_valid[0])\n",
    "\n",
    "            eval_metric_ops = {\"accuracy\": accuracy_valid}\n",
    "\n",
    "            eval_summary_hook = tf.train.SummarySaverHook(\n",
    "                save_steps=1,\n",
    "                output_dir='models/q2-train',\n",
    "                summary_op=tf.summary.merge_all())\n",
    "\n",
    "            return tf.estimator.EstimatorSpec(\n",
    "                mode=mode, \n",
    "                loss=total_loss, \n",
    "                eval_metric_ops=eval_metric_ops,\n",
    "                training_hooks=[eval_summary_hook])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Main function for building, training and evaluating model.\n",
    "def main(train_data, train_labels, eval_data, eval_labels, test_data):\n",
    "    \n",
    "    estimator_dir = 'models/cnn'\n",
    "    \n",
    "    # Delete directory containing events logs and checkpoints if it exists.\n",
    "    if tf.gfile.Exists(estimator_dir):\n",
    "        tf.gfile.DeleteRecursively(estimator_dir)\n",
    "        \n",
    "    # Create directory containing events logs and checkpoints.\n",
    "    tf.gfile.MakeDirs(estimator_dir)\n",
    "    \n",
    "    # Create the Estimator.\n",
    "    classifier = tf.estimator.Estimator(\n",
    "        model_fn=cnn_model_fn, \n",
    "        model_dir=estimator_dir)\n",
    "\n",
    "    for _ in range(10):\n",
    "        \n",
    "        # Train the model.\n",
    "        train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"x\": train_data},\n",
    "            y=train_labels,\n",
    "            batch_size=128,\n",
    "            num_epochs=None,\n",
    "            shuffle=True)\n",
    "\n",
    "        classifier.train(\n",
    "            input_fn=train_input_fn,\n",
    "            steps=100)\n",
    "\n",
    "        # Evaluate the model and print results.\n",
    "        eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"x\": eval_data},\n",
    "            y=eval_labels,\n",
    "            num_epochs=1,\n",
    "            shuffle=False)\n",
    "    \n",
    "        eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "        print(eval_results)\n",
    "        \n",
    "    # Generate predictions on test set.\n",
    "    predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": test_data},\n",
    "      num_epochs=1,\n",
    "      shuffle=False)\n",
    "\n",
    "    predictions = np.array(list(classifier.predict(input_fn=predict_input_fn))).T\n",
    "    \n",
    "    return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load training and validation data\n",
    "train_dir = 'data/spectrogram/training/'\n",
    "valid_dir = 'data/spectrogram/validation/'\n",
    "\n",
    "X_train, Y_train = get_data(os.path.join(train_dir, 'subspec'), \n",
    "                            os.path.join(train_dir, 'REFERENCE.csv'))\n",
    "                            \n",
    "X_valid, Y_valid = get_data(os.path.join(valid_dir, 'subspec'), \n",
    "                            os.path.join(valid_dir, 'REFERENCE.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_save_summary_steps': 100, '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_steps': None, '_model_dir': 'models-test-estimator/', '_tf_random_seed': 1, '_keep_checkpoint_max': 5, '_session_config': None, '_log_step_count_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:loss = 60.3213, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 72 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 100 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.895572.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-19-22:33:41\n",
      "INFO:tensorflow:Restoring parameters from models-test-estimator/model.ckpt-100\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-19-22:34:02\n",
      "INFO:tensorflow:Saving dict for global step 100: accuracy = 0.720848, global_step = 100, loss = 1.18861\n",
      "{'loss': 1.1886137, 'global_step': 100, 'accuracy': 0.72084808}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models-test-estimator/model.ckpt-100\n",
      "INFO:tensorflow:Saving checkpoints for 101 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.900437, step = 101\n",
      "INFO:tensorflow:Saving checkpoints for 131 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 200 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.273767.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-20-00:18:03\n",
      "INFO:tensorflow:Restoring parameters from models-test-estimator/model.ckpt-200\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-20-00:18:26\n",
      "INFO:tensorflow:Saving dict for global step 200: accuracy = 0.797703, global_step = 200, loss = 0.516123\n",
      "{'loss': 0.51612258, 'global_step': 200, 'accuracy': 0.79770321}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models-test-estimator/model.ckpt-200\n",
      "INFO:tensorflow:Saving checkpoints for 201 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.27112, step = 201\n",
      "INFO:tensorflow:Saving checkpoints for 261 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 300 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.273135.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-20-00:35:26\n",
      "INFO:tensorflow:Restoring parameters from models-test-estimator/model.ckpt-300\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-20-00:35:55\n",
      "INFO:tensorflow:Saving dict for global step 300: accuracy = 0.789753, global_step = 300, loss = 0.55483\n",
      "{'loss': 0.55483007, 'global_step': 300, 'accuracy': 0.78975266}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models-test-estimator/model.ckpt-300\n",
      "INFO:tensorflow:Saving checkpoints for 301 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.186627, step = 301\n",
      "INFO:tensorflow:Saving checkpoints for 363 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 400 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.261198.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-20-00:51:48\n",
      "INFO:tensorflow:Restoring parameters from models-test-estimator/model.ckpt-400\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-20-00:52:11\n",
      "INFO:tensorflow:Saving dict for global step 400: accuracy = 0.742933, global_step = 400, loss = 0.545087\n",
      "{'loss': 0.54508698, 'global_step': 400, 'accuracy': 0.74293286}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models-test-estimator/model.ckpt-400\n",
      "INFO:tensorflow:Saving checkpoints for 401 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.299955, step = 401\n",
      "INFO:tensorflow:Saving checkpoints for 468 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 500 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.257225.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-20-01:07:35\n",
      "INFO:tensorflow:Restoring parameters from models-test-estimator/model.ckpt-500\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-20-01:08:00\n",
      "INFO:tensorflow:Saving dict for global step 500: accuracy = 0.806537, global_step = 500, loss = 0.515962\n",
      "{'loss': 0.51596189, 'global_step': 500, 'accuracy': 0.80653709}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models-test-estimator/model.ckpt-500\n",
      "INFO:tensorflow:Saving checkpoints for 501 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.228074, step = 501\n",
      "INFO:tensorflow:Saving checkpoints for 561 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 600 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.211994.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-20-01:23:49\n",
      "INFO:tensorflow:Restoring parameters from models-test-estimator/model.ckpt-600\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-20-01:24:10\n",
      "INFO:tensorflow:Saving dict for global step 600: accuracy = 0.805654, global_step = 600, loss = 0.519708\n",
      "{'loss': 0.5197084, 'global_step': 600, 'accuracy': 0.80565369}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models-test-estimator/model.ckpt-600\n",
      "INFO:tensorflow:Saving checkpoints for 601 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.199173, step = 601\n",
      "INFO:tensorflow:Saving checkpoints for 670 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 700 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.180829.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-20-01:39:29\n",
      "INFO:tensorflow:Restoring parameters from models-test-estimator/model.ckpt-700\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-20-01:39:57\n",
      "INFO:tensorflow:Saving dict for global step 700: accuracy = 0.742933, global_step = 700, loss = 0.641556\n",
      "{'loss': 0.64155644, 'global_step': 700, 'accuracy': 0.74293286}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models-test-estimator/model.ckpt-700\n",
      "INFO:tensorflow:Saving checkpoints for 701 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.206732, step = 701\n",
      "INFO:tensorflow:Saving checkpoints for 757 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 800 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.293789.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-20-01:59:18\n",
      "INFO:tensorflow:Restoring parameters from models-test-estimator/model.ckpt-800\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-20-01:59:45\n",
      "INFO:tensorflow:Saving dict for global step 800: accuracy = 0.766784, global_step = 800, loss = 0.604053\n",
      "{'loss': 0.60405266, 'global_step': 800, 'accuracy': 0.76678443}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models-test-estimator/model.ckpt-800\n",
      "INFO:tensorflow:Saving checkpoints for 801 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.199741, step = 801\n",
      "INFO:tensorflow:Saving checkpoints for 858 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 900 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.189586.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-20-02:18:20\n",
      "INFO:tensorflow:Restoring parameters from models-test-estimator/model.ckpt-900\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-20-02:18:51\n",
      "INFO:tensorflow:Saving dict for global step 900: accuracy = 0.795936, global_step = 900, loss = 0.613583\n",
      "{'loss': 0.61358339, 'global_step': 900, 'accuracy': 0.79593641}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models-test-estimator/model.ckpt-900\n",
      "INFO:tensorflow:Saving checkpoints for 901 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.207821, step = 901\n",
      "INFO:tensorflow:Saving checkpoints for 958 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.275022.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-20-02:36:28\n",
      "INFO:tensorflow:Restoring parameters from models-test-estimator/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-20-02:36:52\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.783569, global_step = 1000, loss = 0.608566\n",
      "{'loss': 0.60856622, 'global_step': 1000, 'accuracy': 0.78356892}\n",
      "INFO:tensorflow:Restoring parameters from models-test-estimator/model.ckpt-1000\n"
     ]
    }
   ],
   "source": [
    "# Run network\n",
    "pred = main(X_train, Y_train, X_valid, Y_valid, X_valid)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
