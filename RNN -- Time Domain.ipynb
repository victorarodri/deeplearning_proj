{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "import csv\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Access data\n",
    "\n",
    "def get_data(data_dir, labels_path):\n",
    "    data_filenames = glob.glob(os.path.join(data_dir, '*npy'))\n",
    "    random.shuffle(data_filenames)\n",
    "\n",
    "    num_examples = len(data_filenames)\n",
    "\n",
    "    max_length = 0\n",
    "    for i, df in enumerate(data_filenames):\n",
    "        max_length = max(max_length, np.load(df).shape[0])\n",
    "    \n",
    "    labels_dict = {}\n",
    "    get_labels_dict(labels_path, labels_dict)\n",
    "    \n",
    "    X = np.zeros([num_examples, max_length])\n",
    "    Y = np.zeros(num_examples)\n",
    "    for i, df in enumerate(data_filenames):\n",
    "        data = np.load(df)\n",
    "        X[i, :data.shape[0]] = data\n",
    "        \n",
    "        label_key = df.split('/')[-1].split('.')[0].split('_')[0]\n",
    "        Y[i] = labels_dict[label_key]\n",
    "\n",
    "    # Convert -1 labels to 0\n",
    "    Y[np.where(Y == -1)] = 0\n",
    "        \n",
    "    return X.astype(np.float32), Y.astype(np.float32)\n",
    "\n",
    "\n",
    "def get_random_data(data_dir, labels_path, num_examples):\n",
    "    data_filenames = glob.glob(os.path.join(data_dir, '*npy'))\n",
    "    random.shuffle(data_filenames)\n",
    "\n",
    "    max_length = 0\n",
    "    for i in range(num_examples):\n",
    "        df = data_filenames[i]\n",
    "        max_length = max(max_length, np.load(df).shape[0])\n",
    "    \n",
    "    labels_dict = {}\n",
    "    get_labels_dict(labels_path, labels_dict)\n",
    "    \n",
    "    X = np.zeros([num_examples, max_length])\n",
    "    Y = np.zeros(num_examples)\n",
    "    for i in range(num_examples):\n",
    "        df = data_filenames[i]\n",
    "        \n",
    "        data = np.load(df)\n",
    "        X[i, :data.shape[0]] = data\n",
    "        \n",
    "        label_key = df.split('/')[-1].split('.')[0].split('_')[0]\n",
    "        Y[i] = labels_dict[label_key]\n",
    "\n",
    "    # Convert -1 labels to 0\n",
    "    Y[np.where(Y == -1)] = 0\n",
    "        \n",
    "    return X.astype(np.float32), Y.astype(np.float32)\n",
    "\n",
    "\n",
    "def get_labels_dict(reference_path, reference):\n",
    "    with open(reference_path) as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "        for row in spamreader:\n",
    "            reference[row[0]] = row[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Methods to account for variable sequence lengths (call for each batch)\n",
    "def length(sequence):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), 2))\n",
    "    length = tf.reduce_sum(used, 1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length\n",
    "\n",
    "def last_relevant(output, length):\n",
    "    batch_size = tf.shape(output)[0]\n",
    "    max_length = tf.shape(output)[1]\n",
    "    out_size = int(output.get_shape()[2])\n",
    "    index = tf.range(0, batch_size) * max_length + (length - 1)\n",
    "    flat = tf.reshape(output, [-1, out_size])\n",
    "    relevant = tf.gather(flat, index)\n",
    "    \n",
    "    return relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def _variable_on_cpu(name, shape, initializer):\n",
    "    \"\"\"\n",
    "    Helper to create a Variable stored on CPU memory.\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "        var = tf.get_variable(\n",
    "            name=name, \n",
    "            shape=shape, \n",
    "            dtype=tf.float32, \n",
    "            initializer=initializer)\n",
    "\n",
    "    return var\n",
    "\n",
    "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "    \"\"\"\n",
    "    Helper to create an initialized Variable with weight decay.\n",
    "    \"\"\"\n",
    "\n",
    "    var = _variable_on_cpu(\n",
    "        name=name,\n",
    "        shape=shape,\n",
    "        initializer=tf.truncated_normal_initializer(\n",
    "            stddev=stddev,\n",
    "            dtype=tf.float32))\n",
    "\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, \n",
    "                                   name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)\n",
    "\n",
    "    return var\n",
    "\n",
    "\n",
    "def loss(unscale_logits, labels):\n",
    "    \"\"\"\n",
    "    Add L2Loss to all the trainable variables.\n",
    "    \"\"\"\n",
    "\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=labels,\n",
    "        logits=unscale_logits,\n",
    "        name='cross_entropy')\n",
    "\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "\n",
    "    tf.add_to_collection('losses', cross_entropy_mean)\n",
    "\n",
    "    total_loss = tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn(features):\n",
    "    \n",
    "    # Resize batch to maximum sequence length within batch\n",
    "    features = features['x']\n",
    "    batch_max_length = tf.reduce_max(length((features[:, :, None])))\n",
    "    features = features[:, :batch_max_length]\n",
    "    \n",
    "    # Input Layer\n",
    "    input_layer = tf.reshape(features, [-1, batch_max_length, 1])\n",
    "\n",
    "    # RNN layer #1\n",
    "    with tf.variable_scope(name_or_scope='rnn1') as scope:\n",
    "        n_units = 300\n",
    "        rnn_cell = tf.nn.rnn_cell.BasicRNNCell(n_units, activation=tf.nn.relu)\n",
    "\n",
    "        _, rnn_state = tf.nn.dynamic_rnn(rnn_cell, \n",
    "                                           inputs=input_layer,\n",
    "                                           #initial_state=initial_state,\n",
    "                                           dtype=tf.float32,\n",
    "                                           sequence_length=length(input_layer),\n",
    "                                           time_major=False,\n",
    "                                           scope=scope.name)\n",
    "    \n",
    "    # Fully connected layer #1.\n",
    "    with tf.variable_scope(name_or_scope='fc1') as scope:\n",
    "\n",
    "        weights = _variable_with_weight_decay(\n",
    "            name='weights',\n",
    "            shape=[n_units, 2], # needs to match output dimension of RNN\n",
    "            stddev=0.04,\n",
    "            wd=0.004)\n",
    "\n",
    "        biases = _variable_on_cpu(\n",
    "            name='biases',\n",
    "            shape=[2], # needs to match output dimension of RNN\n",
    "            initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "        pre_activation = tf.add(tf.matmul(rnn_state, weights), biases)\n",
    "\n",
    "        fc1 = tf.nn.relu(\n",
    "            features=pre_activation,\n",
    "            name=scope.name)\n",
    "\n",
    "        return fc1\n",
    "\n",
    "# def train_neural_network(): \n",
    "#     prediction = recurrent_neural_network(x_placeholder)\n",
    "#     cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=tf.reshape(y_placeholder, [batch_size, n_classes])))\n",
    "#     optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "#     epoch_batch_itr = 10\n",
    "    \n",
    "#     correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_placeholder, 1))\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correct, 'float32'))\n",
    "        \n",
    "#     with tf.Session() as sess:\n",
    "#         tf.global_variables_initializer().run()\n",
    "\n",
    "#         for epoch in range(hm_epochs):\n",
    "#             epoch_loss = 0\n",
    "#             for b in range(epoch_batch_itr):\n",
    "#                 batchX, batchY = generate_batch(train_path)\n",
    "#                 _, c = sess.run([optimizer, cost], feed_dict={x_placeholder: batchX, y_placeholder: batchY})\n",
    "#                 epoch_loss += c\n",
    "#             validation_batchX, validation_batchY = generate_batch(validation_path) \n",
    "#             print('Epoch', epoch, 'loss:', epoch_loss, 'Validation Accuracy:', accuracy.eval({x_placeholder: validation_batchX, y_placeholder: validation_batchY}))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def rnn_model_fn(features, labels, mode):\n",
    "\n",
    "    \"\"\"\n",
    "    Build model.\n",
    "    \"\"\"\n",
    "    unscale_logits = rnn(features)\n",
    "    \n",
    "    # Generate predictions for PREDICT and EVAL modes.\n",
    "    predictions = {\n",
    "        'classes': tf.argmax(input=unscale_logits, axis=1),\n",
    "        'probabilities': tf.nn.softmax(unscale_logits, name='softmax_tensor')\n",
    "    }\n",
    "    \n",
    "    ####################\n",
    "    # PREDICT\n",
    "    ####################\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT: \n",
    "        return tf.estimator.EstimatorSpec(mode=mode, \n",
    "                                          predictions=predictions['probabilities'])\n",
    "    \n",
    "    else:\n",
    "        # Calculate loss for both TRAIN and EVAL modes.\n",
    "        labels = tf.cast(labels, tf.int64)\n",
    "        total_loss = loss(unscale_logits, labels) #REFORM THIS LOSS FUNCTION\n",
    "\n",
    "        # Add summary operation for total loss visualizaiton.\n",
    "        tf.summary.scalar(\n",
    "            name='total_loss',\n",
    "            tensor=total_loss)\n",
    "        \n",
    "        \n",
    "        ####################\n",
    "        # TRAIN\n",
    "        ####################\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            \n",
    "            # Compute gradients using Gradient Descent Optimizer.\n",
    "            optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "            grads_vars = optimizer.compute_gradients(loss=total_loss)\n",
    "\n",
    "            # Add summary operations for gradient visualizations.\n",
    "            for grad, var in grads_vars:\n",
    "                if grad is not None:\n",
    "                    tf.summary.histogram(\n",
    "                        name=var.op.name + '/gradients', \n",
    "                        values=grad)\n",
    "\n",
    "            train_op = optimizer.minimize(\n",
    "                loss=total_loss,\n",
    "                global_step=tf.train.get_global_step())\n",
    "\n",
    "            # Add evaluation metrics for TRAIN mode.\n",
    "            accuracy_train = tf.metrics.accuracy(\n",
    "                labels=labels, \n",
    "                predictions=predictions[\"classes\"])\n",
    "\n",
    "            # Add summary operation for training accuracy visualizaiton.\n",
    "            tf.summary.scalar(\n",
    "                name='accuracy_train',\n",
    "                tensor=accuracy_train[0])\n",
    "\n",
    "            train_summary_hook = tf.train.SummarySaverHook(\n",
    "                save_steps=10,\n",
    "                output_dir='models/rnn',\n",
    "                summary_op=tf.summary.merge_all())\n",
    "\n",
    "            return tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=total_loss, \n",
    "                train_op=train_op,\n",
    "                training_hooks=[train_summary_hook])\n",
    "        \n",
    "        \n",
    "        ####################\n",
    "        # EVALUATE\n",
    "        ####################\n",
    "        else:\n",
    "            accuracy_valid = tf.metrics.accuracy(\n",
    "                labels=labels, \n",
    "                predictions=predictions[\"classes\"])\n",
    "\n",
    "            # Add summary operation for validation accuracy visualizaiton.\n",
    "            tf.summary.scalar(\n",
    "                name='accuracy_validation',\n",
    "                tensor=accuracy_valid[0])\n",
    "\n",
    "            eval_metric_ops = {\"accuracy\": accuracy_valid}\n",
    "\n",
    "            eval_summary_hook = tf.train.SummarySaverHook(\n",
    "                save_steps=1,\n",
    "                output_dir='models/rnn',\n",
    "                summary_op=tf.summary.merge_all())\n",
    "\n",
    "            return tf.estimator.EstimatorSpec(\n",
    "                mode=mode, \n",
    "                loss=total_loss, \n",
    "                eval_metric_ops=eval_metric_ops,\n",
    "                training_hooks=[eval_summary_hook])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Main function for building, training and evaluating model.\n",
    "def main(train_data, train_labels, eval_data, eval_labels, test_data):\n",
    "    \n",
    "    estimator_dir = 'models/rnn'\n",
    "    \n",
    "    # Delete directory containing events logs and checkpoints if it exists.\n",
    "    if tf.gfile.Exists(estimator_dir):\n",
    "        tf.gfile.DeleteRecursively(estimator_dir)\n",
    "\n",
    "    # Create directory containing events logs and checkpoints.\n",
    "    tf.gfile.MakeDirs(estimator_dir)\n",
    "    \n",
    "    # Create the Estimator.\n",
    "    classifier = tf.estimator.Estimator(\n",
    "        model_fn=rnn_model_fn, \n",
    "        model_dir=estimator_dir)\n",
    "\n",
    "    for _ in range(10):\n",
    "        \n",
    "        # Train the model.\n",
    "        train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"x\": train_data},\n",
    "            y=train_labels,\n",
    "            batch_size=5,\n",
    "            num_epochs=1,\n",
    "            shuffle=True)\n",
    "\n",
    "        classifier.train(\n",
    "            input_fn=train_input_fn,\n",
    "            steps=20)\n",
    "\n",
    "        # Evaluate the model and print results.\n",
    "        eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"x\": eval_data},\n",
    "            y=eval_labels,\n",
    "#             batch_size=,\n",
    "            num_epochs=1,\n",
    "            shuffle=False)\n",
    "    \n",
    "        eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "        print(eval_results)\n",
    "        \n",
    "    # Generate predictions on test set.\n",
    "    predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": test_data},\n",
    "      num_epochs=1,\n",
    "      shuffle=False)\n",
    "\n",
    "    predictions = np.array(list(classifier.predict(input_fn=predict_input_fn))).T\n",
    "    \n",
    "    return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_dir = 'data/sequence'\n",
    "\n",
    "# X_train, Y_train = get_data(os.path.join(data_dir, 'training/sub'), \n",
    "#                             os.path.join(data_dir, 'REFERENCE.csv'))\n",
    "                            \n",
    "# X_valid, Y_valid = get_data(os.path.join(data_dir, 'validation/sub'), \n",
    "#                             os.path.join(data_dir, 'REFERENCE.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.0\n"
     ]
    }
   ],
   "source": [
    "# Load a small random data set and try to overfit\n",
    "X_train, Y_train = get_random_data(data_dir='/Volumes/light/deeplearning_proj/data/sequence/validation/sub',\n",
    "                                   labels_path='/Volumes/light/deeplearning_proj/data/sequence/REFERENCE.csv',\n",
    "                                   num_examples=100)\n",
    "                            \n",
    "X_valid, Y_valid = X_train, Y_train\n",
    "\n",
    "print(Y_train.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'models/rnn', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into models/rnn/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.68567, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 20 into models/rnn/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.69439.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-03-19:09:10\n",
      "INFO:tensorflow:Restoring parameters from models/rnn/model.ckpt-20\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-03-19:09:15\n",
      "INFO:tensorflow:Saving dict for global step 20: accuracy = 0.46, global_step = 20, loss = 0.694385\n",
      "{'accuracy': 0.46000001, 'loss': 0.69438541, 'global_step': 20}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models/rnn/model.ckpt-20\n",
      "INFO:tensorflow:Saving checkpoints for 21 into models/rnn/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.694385, step = 21\n",
      "INFO:tensorflow:Saving checkpoints for 40 into models/rnn/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.694331.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-03-19:10:27\n",
      "INFO:tensorflow:Restoring parameters from models/rnn/model.ckpt-40\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-03-19:10:31\n",
      "INFO:tensorflow:Saving dict for global step 40: accuracy = 0.46, global_step = 40, loss = 0.694329\n",
      "{'accuracy': 0.46000001, 'loss': 0.69432944, 'global_step': 40}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models/rnn/model.ckpt-40\n",
      "INFO:tensorflow:Saving checkpoints for 41 into models/rnn/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.694329, step = 41\n",
      "INFO:tensorflow:Saving checkpoints for 60 into models/rnn/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.694309.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-03-19:11:41\n",
      "INFO:tensorflow:Restoring parameters from models/rnn/model.ckpt-60\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-03-19:11:46\n",
      "INFO:tensorflow:Saving dict for global step 60: accuracy = 0.46, global_step = 60, loss = 0.694309\n",
      "{'accuracy': 0.46000001, 'loss': 0.69430912, 'global_step': 60}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models/rnn/model.ckpt-60\n",
      "INFO:tensorflow:Saving checkpoints for 61 into models/rnn/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.694309, step = 61\n",
      "INFO:tensorflow:Saving checkpoints for 80 into models/rnn/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.694302.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-03-19:12:56\n",
      "INFO:tensorflow:Restoring parameters from models/rnn/model.ckpt-80\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-03-19:13:00\n",
      "INFO:tensorflow:Saving dict for global step 80: accuracy = 0.46, global_step = 80, loss = 0.694302\n",
      "{'accuracy': 0.46000001, 'loss': 0.69430208, 'global_step': 80}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models/rnn/model.ckpt-80\n",
      "INFO:tensorflow:Saving checkpoints for 81 into models/rnn/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.694302, step = 81\n",
      "INFO:tensorflow:Saving checkpoints for 100 into models/rnn/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.694299.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-03-19:14:11\n",
      "INFO:tensorflow:Restoring parameters from models/rnn/model.ckpt-100\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-03-19:14:15\n",
      "INFO:tensorflow:Saving dict for global step 100: accuracy = 0.46, global_step = 100, loss = 0.694299\n",
      "{'accuracy': 0.46000001, 'loss': 0.69429916, 'global_step': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models/rnn/model.ckpt-100\n",
      "INFO:tensorflow:Saving checkpoints for 101 into models/rnn/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.694299, step = 101\n",
      "INFO:tensorflow:Saving checkpoints for 120 into models/rnn/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.694297.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-03-19:15:26\n",
      "INFO:tensorflow:Restoring parameters from models/rnn/model.ckpt-120\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-03-19:15:30\n",
      "INFO:tensorflow:Saving dict for global step 120: accuracy = 0.46, global_step = 120, loss = 0.694297\n",
      "{'accuracy': 0.46000001, 'loss': 0.69429731, 'global_step': 120}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models/rnn/model.ckpt-120\n",
      "INFO:tensorflow:Saving checkpoints for 121 into models/rnn/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.694297, step = 121\n",
      "INFO:tensorflow:Saving checkpoints for 140 into models/rnn/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.694295.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-03-19:16:41\n",
      "INFO:tensorflow:Restoring parameters from models/rnn/model.ckpt-140\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-03-19:16:45\n",
      "INFO:tensorflow:Saving dict for global step 140: accuracy = 0.46, global_step = 140, loss = 0.694296\n",
      "{'accuracy': 0.46000001, 'loss': 0.69429559, 'global_step': 140}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models/rnn/model.ckpt-140\n",
      "INFO:tensorflow:Saving checkpoints for 141 into models/rnn/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.694295, step = 141\n",
      "INFO:tensorflow:Saving checkpoints for 160 into models/rnn/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.694294.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-03-19:17:56\n",
      "INFO:tensorflow:Restoring parameters from models/rnn/model.ckpt-160\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-03-19:18:00\n",
      "INFO:tensorflow:Saving dict for global step 160: accuracy = 0.46, global_step = 160, loss = 0.694294\n",
      "{'accuracy': 0.46000001, 'loss': 0.69429386, 'global_step': 160}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models/rnn/model.ckpt-160\n",
      "INFO:tensorflow:Saving checkpoints for 161 into models/rnn/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.694294, step = 161\n",
      "INFO:tensorflow:Saving checkpoints for 180 into models/rnn/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.694292.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-03-19:19:12\n",
      "INFO:tensorflow:Restoring parameters from models/rnn/model.ckpt-180\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-03-19:19:16\n",
      "INFO:tensorflow:Saving dict for global step 180: accuracy = 0.46, global_step = 180, loss = 0.694292\n",
      "{'accuracy': 0.46000001, 'loss': 0.69429213, 'global_step': 180}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models/rnn/model.ckpt-180\n",
      "INFO:tensorflow:Saving checkpoints for 181 into models/rnn/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.694292, step = 181\n",
      "INFO:tensorflow:Saving checkpoints for 200 into models/rnn/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.69429.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-03-19:20:30\n",
      "INFO:tensorflow:Restoring parameters from models/rnn/model.ckpt-200\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-03-19:20:34\n",
      "INFO:tensorflow:Saving dict for global step 200: accuracy = 0.46, global_step = 200, loss = 0.69429\n",
      "{'accuracy': 0.46000001, 'loss': 0.69429034, 'global_step': 200}\n",
      "INFO:tensorflow:Restoring parameters from models/rnn/model.ckpt-200\n"
     ]
    }
   ],
   "source": [
    "pred = main(X_train, Y_train, X_valid, Y_valid, X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_batch(path, batch_size):\n",
    "#     filenames = glob.glob(os.path.join(path, '*npy'))\n",
    "#     return random.sample(filenames, batch_size)\n",
    "\n",
    "# def get_batch_max_len(batch_files):\n",
    "#     batch_max_len=0\n",
    "#     for f in batch_files:\n",
    "#         batch_max_len=max(batch_max_len, np.load(f).shape[0])\n",
    "#     return batch_max_len\n",
    "\n",
    "# def get_reference(reference_path, reference):\n",
    "#     with open(reference_path) as csvfile:\n",
    "#         spamreader=csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "#         for row in spamreader:\n",
    "#             reference[row[0]]=row[1]\n",
    "\n",
    "# def generate_batch(path):\n",
    "#     batch_files=get_batch(path, batch_size)\n",
    "   \n",
    "#     max_seq=get_batch_max_len(batch_files)\n",
    "\n",
    "#     batchX=np.zeros((batch_size, 1, max_seq), dtype='float32')\n",
    "#     i=0\n",
    "#     for f in batch_files:\n",
    "#         arr=np.load(f)\n",
    "#         arr=np.asmatrix(arr)\n",
    "#         batchX[i][0][0:arr.shape[1]]=arr\n",
    "\n",
    "#     i=0\n",
    "#     batchY=np.zeros(batch_size, dtype='int32')\n",
    "#     for f in batch_files:\n",
    "#         batchY[i] = reference_train[f.split('/')[-1].split('.')[0]]\n",
    "#         i += 1\n",
    "    \n",
    "#     # reshape batchX\n",
    "#     batchX = np.swapaxes(batchX, 1, 2)\n",
    "#     batchX = np.swapaxes(batchX, 0, 1)\n",
    "    \n",
    "#     # convert batchY to onehot\n",
    "#     y_onehot = np.zeros((batch_size, 2))\n",
    "#     y_onehot[np.where(batchY==1)] = np.array([0,1])\n",
    "#     y_onehot[np.where(batchY==-1)] = np.array([1,0])\n",
    "    \n",
    "#     print('batchX, y_onehot, sizes', batchX.shape, y_onehot.shape)\n",
    "    \n",
    "#     return batchX, y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # define variables\n",
    "# train_path='./data/td/training/'\n",
    "# validation_path='./data/td/validation/'\n",
    "# reference_train_path=train_path + 'REFERENCE.csv'\n",
    "# reference_validation_path=validation_path + 'REFERENCE.csv'\n",
    "# train_dirs=os.listdir(train_path)\n",
    "# train_size=len(train_dirs)\n",
    "# batch_size=20\n",
    "\n",
    "# batch_train_files=random.sample(os.listdir(train_path), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hm_epochs = 5\n",
    "# n_classes = 2\n",
    "# state_size = 3\n",
    "# n_units = 30\n",
    "\n",
    "# x_placeholder = tf.placeholder('float32', [None, batch_size, 1])\n",
    "# y_placeholder = tf.placeholder('int32', [batch_size, n_classes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def find_maxlength_file(data_dir):\n",
    "#     data_filenames = glob.glob(os.path.join(data_dir, '*npy'))\n",
    "#     random.shuffle(data_filenames)\n",
    "\n",
    "#     num_examples = len(data_filenames)\n",
    "\n",
    "#     max_length = 0\n",
    "#     max_length_fn = '_'\n",
    "#     for i, df in enumerate(data_filenames):\n",
    "#         length = np.load(df).shape[0]\n",
    "#         if length > max_length:\n",
    "#             max_length = length\n",
    "#             max_length_df = df\n",
    "            \n",
    "#     return max_length, max_length_df\n",
    "\n",
    "# data_dir = '../deeplearning/data/td/training'\n",
    "\n",
    "# ml, ml_df = find_maxlength_file(data_dir)\n",
    "# print('ML: {}'.format(ml))\n",
    "# print('ML file: {}'.format(ml_df))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
