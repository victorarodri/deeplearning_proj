{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import glob\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Access data\n",
    "def get_data(data_dir, labels_path):\n",
    "    data_filenames = glob.glob(os.path.join(data_dir, '*npy'))\n",
    "    random.shuffle(data_filenames)\n",
    "\n",
    "    num_examples = len(data_filenames)\n",
    "    \n",
    "    labels_dict = {}\n",
    "    get_labels_dict(labels_path, labels_dict)\n",
    "    \n",
    "    X = np.zeros([num_examples, 151, 100])\n",
    "    Y = np.zeros(num_examples)\n",
    "    for i, df in enumerate(data_filenames):\n",
    "        X[i, :, :] = np.load(df)\n",
    "        \n",
    "        label_key = df.split('/')[-1].split('.')[0].split('_')[0]\n",
    "        Y[i] = labels_dict[label_key]\n",
    "        \n",
    "    # Convert -1 labels to 0\n",
    "    Y[np.where(Y == -1)] = 0\n",
    "        \n",
    "    return X.astype(np.float32), Y.astype(np.float32)\n",
    "\n",
    "\n",
    "def get_labels_dict(reference_path, reference):\n",
    "    with open(reference_path) as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "        for row in spamreader:\n",
    "            reference[row[0]] = row[1]\n",
    "            \n",
    "    \n",
    "def get_batch(train_path, batch_size):\n",
    "    filenames = glob.glob(os.path.join(train_path, '*npy'))\n",
    "    return random.sample(filenames, batch_size)\n",
    "\n",
    "            \n",
    "def generate_batch(train_path, reference_path, batch_size):\n",
    "    # read in reference train/validation file\n",
    "    reference_train, reference_validation = {}, {}\n",
    "    get_reference(reference_path, reference_train)\n",
    "#     get_reference(reference_validation_path, reference_validation)\n",
    "    \n",
    "    batch_train_files = get_batch(train_path, batch_size)\n",
    "    \n",
    "    batchX = np.zeros([batch_size, 151, 100])\n",
    "    batchY = np.zeros(batch_size)\n",
    "    for i, f in enumerate(batch_train_files):\n",
    "        arr = np.load(f)\n",
    "    \n",
    "        batchX[i, :, :] = arr\n",
    "        batchY[i] = reference_train[f.split('/')[-1].split('.')[0].split('_')[0]]\n",
    "    \n",
    "    # Convert -1 labels to 0\n",
    "    batchY[np.where(batchY == -1)] = 0\n",
    "    \n",
    "    return batchX.astype(np.float32), batchY.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def _variable_on_cpu(name, shape, initializer):\n",
    "    \"\"\"\n",
    "    Helper to create a Variable stored on CPU memory.\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "        var = tf.get_variable(\n",
    "            name=name, \n",
    "            shape=shape, \n",
    "            dtype=tf.float32, \n",
    "            initializer=initializer)\n",
    "\n",
    "    return var\n",
    "\n",
    "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "    \"\"\"\n",
    "    Helper to create an initialized Variable with weight decay.\n",
    "    \"\"\"\n",
    "\n",
    "    var = _variable_on_cpu(\n",
    "        name=name,\n",
    "        shape=shape,\n",
    "        initializer=tf.truncated_normal_initializer(\n",
    "            stddev=stddev,\n",
    "            dtype=tf.float32))\n",
    "\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, \n",
    "                                   name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)\n",
    "\n",
    "    return var\n",
    "\n",
    "\n",
    "def loss(unscale_logits, labels):\n",
    "    \"\"\"\n",
    "    Add L2Loss to all the trainable variables.\n",
    "    \"\"\"\n",
    "\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=labels,\n",
    "        logits=unscale_logits,\n",
    "        name='cross_entropy')\n",
    "\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "\n",
    "    tf.add_to_collection('losses', cross_entropy_mean)\n",
    "\n",
    "    total_loss = tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cnn(features):\n",
    "\n",
    "    # Input Layer.\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 151, 100, 1])\n",
    "    \n",
    "        \n",
    "#     input_layer = tf.map_fn(lambda input: tf.image.per_image_standardization(input), \n",
    "#                             input_layer) #Normalizes all inputs. Subtracts mean and divides by std.\n",
    "\n",
    "    # Convolutional layer #1.\n",
    "    with tf.variable_scope(name_or_scope='conv1') as scope:\n",
    "        kernel = _variable_with_weight_decay(\n",
    "            name='weights',\n",
    "            shape=[5, 5, 1, 64],\n",
    "            stddev=5e-2,\n",
    "            wd=0.0)\n",
    "\n",
    "        conv = tf.nn.conv2d(\n",
    "            input=input_layer,\n",
    "            filter=kernel,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding='SAME')\n",
    "\n",
    "        biases = _variable_on_cpu(\n",
    "            name='biases',\n",
    "            shape=[64],\n",
    "            initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "\n",
    "        conv1 = tf.nn.tanh(\n",
    "            x=pre_activation, \n",
    "            name=scope.name)\n",
    "        \n",
    "        \n",
    "    # Max pooling layer #1.\n",
    "    pool1 = tf.nn.max_pool(\n",
    "        value=conv1,\n",
    "        ksize=[1, 3, 3, 1], \n",
    "        strides=[1, 2, 2, 1],\n",
    "        padding='SAME', \n",
    "        name='pool1')\n",
    "\n",
    "    \n",
    "    # Local response normalization layer #1.\n",
    "    norm1 = tf.nn.lrn(\n",
    "        input=pool1, \n",
    "        depth_radius=4, \n",
    "        bias=1.0, \n",
    "        alpha=0.001 / 9.0, \n",
    "        beta=0.75,\n",
    "        name='norm1')\n",
    "    \n",
    "\n",
    "    # Fully connected layer #1.\n",
    "    with tf.variable_scope(name_or_scope='fc1') as scope:\n",
    "        flat = tf.reshape(norm1, [-1, 243200])\n",
    "\n",
    "        weights = _variable_with_weight_decay(\n",
    "            name='weights',\n",
    "            shape=[243200, 2],\n",
    "            stddev=0.04,\n",
    "            wd=0.004)\n",
    "\n",
    "        biases = _variable_on_cpu(\n",
    "            name='biases',\n",
    "            shape=[2],\n",
    "            initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "        pre_activation = tf.add(tf.matmul(flat, weights), biases)\n",
    "\n",
    "        fc1 = tf.nn.relu(\n",
    "            features=pre_activation,\n",
    "            name=scope.name)\n",
    "        \n",
    "    return fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def cnn_model_fn(features, labels, mode):\n",
    "    \"\"\"\n",
    "    Build CIFAR-10 model.\n",
    "    \"\"\"\n",
    "    fc1 = cnn(features)\n",
    "        \n",
    "    # Generate predictions for PREDICT and EVAL modes.\n",
    "    predictions = {\n",
    "        'classes': tf.argmax(input=fc1, axis=1),\n",
    "        'probabilities': tf.nn.softmax(fc1, name='softmax_tensor')\n",
    "    }\n",
    "    \n",
    "    ####################\n",
    "    # PREDICT\n",
    "    ####################\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT: \n",
    "        return tf.estimator.EstimatorSpec(mode=mode, \n",
    "                                          predictions=predictions['probabilities'])\n",
    "    \n",
    "    else:\n",
    "        # Calculate loss for both TRAIN and EVAL modes.\n",
    "        labels = tf.cast(labels, tf.int64)\n",
    "        total_loss = loss(fc1, labels)\n",
    "\n",
    "        # Add summary operation for total loss visualizaiton.\n",
    "        tf.summary.scalar(\n",
    "            name='total_loss',\n",
    "            tensor=total_loss)\n",
    "    \n",
    "\n",
    "        ####################\n",
    "        # TRAIN\n",
    "        ####################\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            # Update learning rate.\n",
    "    #         num_batches_per_epoch = 50000 / 128\n",
    "    #         decay_steps = int(num_batches_per_epoch * 350.0)\n",
    "\n",
    "    #         lr = tf.train.exponential_decay(\n",
    "    #             learning_rate=0.1,\n",
    "    #             global_step=tf.train.get_global_step(),\n",
    "    #             decay_steps=decay_steps,\n",
    "    #             decay_rate=0.1,\n",
    "    #             staircase=True,\n",
    "    #             name='learning_rate')\n",
    "\n",
    "            # Add summary operation for learning rate visualizaiton.\n",
    "    #         tf.summary.scalar(\n",
    "    #             name='learning_rate', \n",
    "    #             tensor=lr)\n",
    "\n",
    "            # Compute gradients using Gradient Descent Optimizer.\n",
    "            optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "            grads_vars = optimizer.compute_gradients(loss=total_loss)\n",
    "\n",
    "            # Add summary operations for gradient visualizations.\n",
    "            for grad, var in grads_vars:\n",
    "                if grad is not None:\n",
    "                    tf.summary.histogram(\n",
    "                        name=var.op.name + '/gradients', \n",
    "                        values=grad)\n",
    "\n",
    "            train_op = optimizer.minimize(\n",
    "                loss=total_loss,\n",
    "                global_step=tf.train.get_global_step())\n",
    "\n",
    "            # Add evaluation metrics for TRAIN mode.\n",
    "            accuracy_train = tf.metrics.accuracy(\n",
    "                labels=labels, \n",
    "                predictions=predictions[\"classes\"])\n",
    "\n",
    "            # Add summary operation for training accuracy visualizaiton.\n",
    "            tf.summary.scalar(\n",
    "                name='accuracy_train',\n",
    "                tensor=accuracy_train[0])\n",
    "\n",
    "            train_summary_hook = tf.train.SummarySaverHook(\n",
    "                save_steps=10,\n",
    "                output_dir='models/q2-train',\n",
    "                summary_op=tf.summary.merge_all())\n",
    "\n",
    "            return tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=total_loss, \n",
    "                train_op=train_op,\n",
    "                training_hooks=[train_summary_hook])\n",
    "\n",
    "        \n",
    "        ####################\n",
    "        # EVALUATE\n",
    "        ####################\n",
    "        else:\n",
    "            accuracy_valid = tf.metrics.accuracy(\n",
    "                labels=labels, \n",
    "                predictions=predictions[\"classes\"])\n",
    "\n",
    "            # Add summary operation for validation accuracy visualizaiton.\n",
    "            tf.summary.scalar(\n",
    "                name='accuracy_validation',\n",
    "                tensor=accuracy_valid[0])\n",
    "\n",
    "            eval_metric_ops = {\"accuracy\": accuracy_valid}\n",
    "\n",
    "            eval_summary_hook = tf.train.SummarySaverHook(\n",
    "                save_steps=1,\n",
    "                output_dir='models/q2-train',\n",
    "                summary_op=tf.summary.merge_all())\n",
    "\n",
    "            return tf.estimator.EstimatorSpec(\n",
    "                mode=mode, \n",
    "                loss=total_loss, \n",
    "                eval_metric_ops=eval_metric_ops,\n",
    "                training_hooks=[eval_summary_hook])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Main function for building, training and evaluating model.\n",
    "def main(train_data, train_labels, eval_data, eval_labels, test_data):\n",
    "    \n",
    "    estimator_dir = 'models-test-estimator/'\n",
    "    train_dir = 'models-test-train/'\n",
    "    \n",
    "    # Delete directory containing events logs and checkpoints if it exists.\n",
    "    if tf.gfile.Exists(estimator_dir):\n",
    "        tf.gfile.DeleteRecursively(estimator_dir)\n",
    "        \n",
    "    if tf.gfile.Exists(train_dir):\n",
    "        tf.gfile.DeleteRecursively(train_dir)\n",
    "\n",
    "    # Create directory containing events logs and checkpoints.\n",
    "    tf.gfile.MakeDirs(estimator_dir)\n",
    "    tf.gfile.MakeDirs(train_dir)\n",
    "    \n",
    "    # Create the Estimator.\n",
    "    cifar_classifier = tf.estimator.Estimator(\n",
    "        model_fn=cnn_model_fn, \n",
    "        model_dir=estimator_dir)\n",
    "\n",
    "    for _ in range(10):\n",
    "        \n",
    "        # Train the model.\n",
    "        train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"x\": train_data},\n",
    "            y=train_labels,\n",
    "            batch_size=128,\n",
    "            num_epochs=None,\n",
    "            shuffle=True)\n",
    "\n",
    "        cifar_classifier.train(\n",
    "            input_fn=train_input_fn,\n",
    "            steps=100)\n",
    "\n",
    "        # Evaluate the model and print results.\n",
    "        eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"x\": eval_data},\n",
    "            y=eval_labels,\n",
    "            num_epochs=1,\n",
    "            shuffle=False)\n",
    "    \n",
    "        eval_results = cifar_classifier.evaluate(input_fn=eval_input_fn)\n",
    "        print(eval_results)\n",
    "        \n",
    "    # Generate predictions on test set.\n",
    "    predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": test_data},\n",
    "      num_epochs=1,\n",
    "      shuffle=False)\n",
    "\n",
    "    predictions = np.array(list(cifar_classifier.predict(input_fn=predict_input_fn))).T\n",
    "    \n",
    "    return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dir = 'data/spectrogram/training/'\n",
    "valid_dir = 'data/spectrogram/validation/'\n",
    "\n",
    "X_train, Y_train = get_data(os.path.join(train_dir, 'subspec'), \n",
    "                            os.path.join(train_dir, 'REFERENCE.csv'))\n",
    "                            \n",
    "X_valid, Y_valid = get_data(os.path.join(valid_dir, 'subspec'), \n",
    "                            os.path.join(valid_dir, 'REFERENCE.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_log_step_count_steps': 100, '_save_checkpoints_steps': None, '_tf_random_seed': 1, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_secs': 600, '_save_summary_steps': 100, '_session_config': None, '_model_dir': 'models-test-estimator/'}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.41914, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 100 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.70809.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-18-21:09:53\n",
      "INFO:tensorflow:Restoring parameters from models-test-estimator/model.ckpt-100\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-18-21:09:59\n",
      "INFO:tensorflow:Saving dict for global step 100: accuracy = 0.488516, global_step = 100, loss = 1.70546\n",
      "{'loss': 1.7054557, 'accuracy': 0.48851591, 'global_step': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models-test-estimator/model.ckpt-100\n",
      "INFO:tensorflow:Saving checkpoints for 101 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.70546, step = 101\n",
      "INFO:tensorflow:Saving checkpoints for 200 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.45472.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-18-21:14:29\n",
      "INFO:tensorflow:Restoring parameters from models-test-estimator/model.ckpt-200\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-18-21:14:35\n",
      "INFO:tensorflow:Saving dict for global step 200: accuracy = 0.488516, global_step = 200, loss = 1.45236\n",
      "{'loss': 1.4523629, 'accuracy': 0.48851591, 'global_step': 200}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models-test-estimator/model.ckpt-200\n",
      "INFO:tensorflow:Saving checkpoints for 201 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.45236, step = 201\n",
      "INFO:tensorflow:Saving checkpoints for 300 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.24323.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-18-21:19:03\n",
      "INFO:tensorflow:Restoring parameters from models-test-estimator/model.ckpt-300\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-18-21:19:09\n",
      "INFO:tensorflow:Saving dict for global step 300: accuracy = 0.488516, global_step = 300, loss = 1.24138\n",
      "{'loss': 1.241377, 'accuracy': 0.48851591, 'global_step': 300}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models-test-estimator/model.ckpt-300\n",
      "INFO:tensorflow:Saving checkpoints for 301 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.24138, step = 301\n",
      "INFO:tensorflow:Saving checkpoints for 400 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.08229.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-18-21:23:46\n",
      "INFO:tensorflow:Restoring parameters from models-test-estimator/model.ckpt-400\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-18-21:23:52\n",
      "INFO:tensorflow:Saving dict for global step 400: accuracy = 0.488516, global_step = 400, loss = 1.08093\n",
      "{'loss': 1.0809261, 'accuracy': 0.48851591, 'global_step': 400}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models-test-estimator/model.ckpt-400\n",
      "INFO:tensorflow:Saving checkpoints for 401 into models-test-estimator/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.08093, step = 401\n"
     ]
    }
   ],
   "source": [
    "pred = main(X_train, Y_train, X_valid, Y_valid, X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     #Randomly distort images if in TRAIN mode.\n",
    "#     if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "#         input_layer = tf.map_fn(lambda input: tf.random_crop(input, [24, 24, 3]), \n",
    "#                                    input_layer) #Randomly crop image to 24 x 24.\n",
    "        \n",
    "#         input_layer = tf.map_fn(lambda input: tf.image.random_flip_left_right(input), \n",
    "#                                    input_layer) #Randomly flip image from left to right.\n",
    "        \n",
    "#         input_layer = tf.map_fn(lambda input: tf.image.random_brightness(input, max_delta=63),\n",
    "#                                 input_layer)\n",
    "        \n",
    "#         input_layer = tf.map_fn(lambda input: tf.image.random_contrast(input, lower=0.2, upper=1.8),\n",
    "#                                input_layer)\n",
    "        \n",
    "#         input_layer = tf.map_fn(lambda input: tf.image.per_image_standardization(input), \n",
    "#                                 input_layer) #Normalizes all inputs. Subtracts mean and divides by std.\n",
    "        \n",
    "#     else:        \n",
    "#         input_layer = tf.map_fn(lambda input: tf.image.resize_image_with_crop_or_pad(input, 24, 24), \n",
    "#                                    input_layer) #Centrally crop image to 24 x 24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  # Helper functions for loading data\n",
    "\n",
    "# def get_img_array(path):\n",
    "#     \"\"\"\n",
    "#     Given path of image, returns it's numpy array\n",
    "#     \"\"\"\n",
    "#     return scipy.misc.imread(path)\n",
    "\n",
    "\n",
    "# def get_files(folder):\n",
    "#     \"\"\"\n",
    "#     Given path to folder, returns list of files in it\n",
    "#     \"\"\"\n",
    "#     filenames = [file for file in glob.glob(folder+'*/*')]\n",
    "#     filenames.sort()\n",
    "    \n",
    "#     return filenames\n",
    "\n",
    "\n",
    "# def get_label(filepath, label2id):\n",
    "#     \"\"\"\n",
    "#     Files are assumed to be labeled as: /path/to/file/999_frog.png\n",
    "#     Returns label for a filepath\n",
    "#     \"\"\"\n",
    "#     tokens = filepath.split('/')\n",
    "#     label = tokens[-1].split('_')[1][:-4]\n",
    "    \n",
    "#     if label in label2id:\n",
    "#         return label2id[label]\n",
    "#     else:\n",
    "#         sys.exit(\"Invalid label: \" + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Functions to load data\n",
    "\n",
    "# def get_labels(folder, label2id):\n",
    "#     \"\"\"\n",
    "#     Returns vector of labels extracted from filenames of all files in folder\n",
    "#     :param folder: path to data folder\n",
    "#     :param label2id: mapping of text labels to numeric ids. (Eg: automobile -> 0)\n",
    "#     \"\"\"\n",
    "#     files = get_files(folder)\n",
    "#     y = []\n",
    "    \n",
    "#     for f in files:\n",
    "#         y.append(get_label(f,label2id))\n",
    "#     return np.array(y)\n",
    "\n",
    "\n",
    "# def get_label_mapping(label_file):\n",
    "#     \"\"\"\n",
    "#     Returns mappings of label to index and index to label\n",
    "#     The input file has list of labels, each on a separate line.\n",
    "#     \"\"\"\n",
    "#     with open(label_file, 'r') as f:\n",
    "#         id2label = f.readlines()\n",
    "#         id2label = [l.strip() for l in id2label]\n",
    "#     label2id = {}\n",
    "#     count = 0\n",
    "    \n",
    "#     for label in id2label:\n",
    "#         label2id[label] = count\n",
    "#         count += 1\n",
    "        \n",
    "#     return id2label, label2id\n",
    "\n",
    "\n",
    "# def get_images(folder):\n",
    "#     \"\"\"\n",
    "#     returns numpy array of all samples in folder\n",
    "#     each column is a sample resized to 30x30 and flattened\n",
    "#     \"\"\"\n",
    "#     files = get_files(folder)\n",
    "#     images = []\n",
    "#     count = 0\n",
    "    \n",
    "#     for f in files:\n",
    "#         count += 1\n",
    "#         if count % 10000 == 0:\n",
    "#             print(\"Loaded {}/{}\".format(count,len(files)))\n",
    "            \n",
    "#         img_arr = get_img_array(f)\n",
    "                \n",
    "#         img_arr = img_arr.flatten() / 255.0\n",
    "#         images.append(img_arr)\n",
    "        \n",
    "#     X = np.column_stack(images)\n",
    "\n",
    "#     return X\n",
    "\n",
    "# def get_train_data(data_root_path):\n",
    "#     \"\"\"\n",
    "#     Return X and y\n",
    "#     \"\"\"\n",
    "#     train_data_path = data_root_path + 'train'\n",
    "#     id2label, label2id = get_label_mapping(data_root_path+'labels.txt')\n",
    "    \n",
    "#     print(label2id)\n",
    "    \n",
    "#     X = get_images(train_data_path)\n",
    "#     y = get_labels(train_data_path, label2id)\n",
    "    \n",
    "#     return X, y\n",
    "\n",
    "\n",
    "# def save_predictions(filename, y):\n",
    "#     \"\"\"\n",
    "#     Dumps y into .npy file\n",
    "#     \"\"\"\n",
    "#     np.save(filename, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# def _variable_on_cpu(name, shape, initializer):\n",
    "#     \"\"\"\n",
    "#     Helper to create a Variable stored on CPU memory.\n",
    "#     \"\"\"\n",
    "\n",
    "#     with tf.device('/cpu:0'):\n",
    "#         var = tf.get_variable(\n",
    "#             name=name, \n",
    "#             shape=shape, \n",
    "#             dtype=tf.float32, \n",
    "#             initializer=initializer)\n",
    "\n",
    "#     return var\n",
    "\n",
    "# def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "#     \"\"\"\n",
    "#     Helper to create an initialized Variable with weight decay.\n",
    "#     \"\"\"\n",
    "\n",
    "#     var = _variable_on_cpu(\n",
    "#         name=name,\n",
    "#         shape=shape,\n",
    "#         initializer=tf.truncated_normal_initializer(\n",
    "#             stddev=stddev,\n",
    "#             dtype=tf.float32))\n",
    "\n",
    "#     if wd is not None:\n",
    "#         weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "#         tf.add_to_collection('losses', weight_decay)\n",
    "\n",
    "#     return var\n",
    "\n",
    "\n",
    "# def loss(unscale_logits, labels):\n",
    "#     \"\"\"\n",
    "#     Add L2Loss to all the trainable variables.\n",
    "#     \"\"\"\n",
    "\n",
    "#     cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "#         labels=labels,\n",
    "#         logits=unscale_logits,\n",
    "#         name='cross_entropy')\n",
    "\n",
    "#     cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "\n",
    "#     tf.add_to_collection('losses', cross_entropy_mean)\n",
    "\n",
    "#     total_loss = tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "\n",
    "#     return total_loss\n",
    "\n",
    "\n",
    "# def cnn_model_fn(features, labels, mode):\n",
    "#     \"\"\"\n",
    "#     Build CIFAR-10 model.\n",
    "#     \"\"\"\n",
    "#     # Input Layer.\n",
    "#     input_layer = tf.reshape(features[\"x\"], [-1, 151, 100, 1])\n",
    "    \n",
    "# #     #Randomly distort images if in TRAIN mode.\n",
    "# #     if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "# #         input_layer = tf.map_fn(lambda input: tf.random_crop(input, [24, 24, 3]), \n",
    "# #                                    input_layer) #Randomly crop image to 24 x 24.\n",
    "        \n",
    "# #         input_layer = tf.map_fn(lambda input: tf.image.random_flip_left_right(input), \n",
    "# #                                    input_layer) #Randomly flip image from left to right.\n",
    "        \n",
    "# #         input_layer = tf.map_fn(lambda input: tf.image.random_brightness(input, max_delta=63),\n",
    "# #                                 input_layer)\n",
    "        \n",
    "# #         input_layer = tf.map_fn(lambda input: tf.image.random_contrast(input, lower=0.2, upper=1.8),\n",
    "# #                                input_layer)\n",
    "        \n",
    "# #         input_layer = tf.map_fn(lambda input: tf.image.per_image_standardization(input), \n",
    "# #                                 input_layer) #Normalizes all inputs. Subtracts mean and divides by std.\n",
    "        \n",
    "# #     else:        \n",
    "# #         input_layer = tf.map_fn(lambda input: tf.image.resize_image_with_crop_or_pad(input, 24, 24), \n",
    "# #                                    input_layer) #Centrally crop image to 24 x 24.\n",
    "        \n",
    "#     input_layer = tf.map_fn(lambda input: tf.image.per_image_standardization(input), \n",
    "#                             input_layer) #Normalizes all inputs. Subtracts mean and divides by std.\n",
    "    \n",
    "\n",
    "#     # Convolutional layer #1.\n",
    "#     with tf.variable_scope(name_or_scope='conv1') as scope:\n",
    "#         kernel = _variable_with_weight_decay(\n",
    "#             name='weights',\n",
    "#             shape=[5, 5, 1, 64],\n",
    "#             stddev=5e-2,\n",
    "#             wd=0.0)\n",
    "\n",
    "#         conv = tf.nn.conv2d(\n",
    "#             input=input_layer,\n",
    "#             filter=kernel,\n",
    "#             strides=[1, 1, 1, 1],\n",
    "#             padding='SAME')\n",
    "\n",
    "#         biases = _variable_on_cpu(\n",
    "#             name='biases',\n",
    "#             shape=[64],\n",
    "#             initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "#         pre_activation = tf.nn.bias_add(conv, biases)\n",
    "\n",
    "#         conv1 = tf.nn.tanh(\n",
    "#             x=pre_activation, \n",
    "#             name=scope.name)\n",
    "\n",
    "#     # Max pooling layer #1.\n",
    "#     pool1 = tf.nn.max_pool(\n",
    "#         value=conv1,\n",
    "#         ksize=[1, 3, 3, 1], \n",
    "#         strides=[1, 2, 2, 1],\n",
    "#         padding='SAME', \n",
    "#         name='pool1')\n",
    "\n",
    "#     # Local response normalization layer #1.\n",
    "#     norm1 = tf.nn.lrn(\n",
    "#         input=pool1, \n",
    "#         depth_radius=4, \n",
    "#         bias=1.0, \n",
    "#         alpha=0.001 / 9.0, \n",
    "#         beta=0.75,\n",
    "#         name='norm1')\n",
    "\n",
    "#     # Convolutional layer #2.\n",
    "#     with tf.variable_scope(name_or_scope='conv2') as scope:\n",
    "#         kernel = _variable_with_weight_decay(\n",
    "#             name='weights',\n",
    "#             shape=[5, 5, 64, 64],\n",
    "#             stddev=5e-2,\n",
    "#             wd=0.0)\n",
    "\n",
    "#         conv = tf.nn.conv2d(\n",
    "#             input=norm1,\n",
    "#             filter=kernel,\n",
    "#             strides=[1, 1, 1, 1],\n",
    "#             padding='SAME')\n",
    "\n",
    "#         biases = _variable_on_cpu(\n",
    "#             name='biases',\n",
    "#             shape=[64],\n",
    "#             initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "#         pre_activation = tf.nn.bias_add(conv, biases)\n",
    "\n",
    "#         conv2 = tf.nn.tanh(\n",
    "#             x=pre_activation, \n",
    "#             name=scope.name)\n",
    "\n",
    "#     # Local response normalization layer #2.\n",
    "#     norm2 = tf.nn.lrn(\n",
    "#         input=conv2, \n",
    "#         depth_radius=4, \n",
    "#         bias=1.0, \n",
    "#         alpha=0.001 / 9.0, \n",
    "#         beta=0.75,\n",
    "#         name='norm2')\n",
    "\n",
    "#     # Max pooling layer #2.\n",
    "#     pool2 = tf.nn.max_pool(\n",
    "#         value=norm2,\n",
    "#         ksize=[1, 3, 3, 1], \n",
    "#         strides=[1, 2, 2, 1],\n",
    "#         padding='SAME', \n",
    "#         name='pool2')\n",
    "\n",
    "#     # Fully connected layer #1.\n",
    "#     with tf.variable_scope(name_or_scope='fc1') as scope:\n",
    "#         flat = tf.reshape(pool2, [-1, 4096])\n",
    "\n",
    "#         weights = _variable_with_weight_decay(\n",
    "#             name='weights',\n",
    "#             shape=[4096, 384],\n",
    "#             stddev=0.04,\n",
    "#             wd=0.004)\n",
    "\n",
    "#         biases = _variable_on_cpu(\n",
    "#             name='biases',\n",
    "#             shape=[384],\n",
    "#             initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "#         pre_activation = tf.add(tf.matmul(flat, weights), biases)\n",
    "\n",
    "#         fc1 = tf.nn.relu(\n",
    "#             features=pre_activation,\n",
    "#             name=scope.name)\n",
    "\n",
    "#     # Fully connected layer #2.\n",
    "#     with tf.variable_scope(name_or_scope='fc2') as scope:\n",
    "#         weights = _variable_with_weight_decay(\n",
    "#             name='weights',\n",
    "#             shape=[384, 192],\n",
    "#             stddev=0.04,\n",
    "#             wd=0.004)\n",
    "\n",
    "#         biases = _variable_on_cpu(\n",
    "#             name='biases',\n",
    "#             shape=[192],\n",
    "#             initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "#         pre_activation = tf.add(tf.matmul(fc1, weights), biases)\n",
    "\n",
    "#         fc2 = tf.nn.relu(\n",
    "#             features=pre_activation,\n",
    "#             name=scope.name)\n",
    "\n",
    "#     # Unscaled logits layer #1.\n",
    "#     with tf.variable_scope(name_or_scope='logits1') as scope:\n",
    "#         weights = _variable_with_weight_decay(\n",
    "#             name='weights',\n",
    "#             shape=[192, 10],\n",
    "#             stddev=1.0 / 192.0,\n",
    "#             wd=0.0)\n",
    "\n",
    "#         biases = _variable_on_cpu(\n",
    "#             name='biases',\n",
    "#             shape=[10],\n",
    "#             initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "#         unscale_logits = tf.add(tf.matmul(fc2, weights), biases)\n",
    "        \n",
    "        \n",
    "#     predictions = {\n",
    "#         # Generate predictions for PREDICT and EVAL modes.\n",
    "#         'classes': tf.argmax(input=unscale_logits, axis=1),\n",
    "#         'probabilities': tf.nn.softmax(unscale_logits, name='softmax_tensor')\n",
    "#     }\n",
    "    \n",
    "    \n",
    "#     if mode == tf.estimator.ModeKeys.PREDICT: \n",
    "#         return tf.estimator.EstimatorSpec(mode=mode, \n",
    "#                                           predictions=predictions['probabilities'])\n",
    "    \n",
    "#     # Calculate loss for both TRAIN and EVAL modes.\n",
    "#     labels = tf.cast(labels, tf.int64)\n",
    "    \n",
    "#     total_loss = loss(unscale_logits, labels)\n",
    "    \n",
    "#     # Add summary operation for total loss visualizaiton.\n",
    "#     tf.summary.scalar(\n",
    "#         name='total_loss',\n",
    "#         tensor=total_loss)\n",
    "    \n",
    "#     # Configure the Training Op for TRAIN mode.\n",
    "#     if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "#         #Update learning rate.\n",
    "# #         num_batches_per_epoch = 50000 / 128\n",
    "# #         decay_steps = int(num_batches_per_epoch * 350.0)\n",
    "        \n",
    "# #         lr = tf.train.exponential_decay(\n",
    "# #             learning_rate=0.1,\n",
    "# #             global_step=tf.train.get_global_step(),\n",
    "# #             decay_steps=decay_steps,\n",
    "# #             decay_rate=0.1,\n",
    "# #             staircase=True,\n",
    "# #             name='learning_rate')\n",
    "\n",
    "#         # Add summary operation for learning rate visualizaiton.\n",
    "# #         tf.summary.scalar(\n",
    "# #             name='learning_rate', \n",
    "# #             tensor=lr)\n",
    "        \n",
    "#         # Compute gradients using Gradient Descent Optimizer.\n",
    "#         optimizer = tf.train.AdamOptimizer()\n",
    "        \n",
    "#         grads_vars = optimizer.compute_gradients(loss=total_loss)\n",
    "        \n",
    "#         # Add summary operations for gradient visualizations.\n",
    "#         for grad, var in grads_vars:\n",
    "#             if grad is not None:\n",
    "#                 tf.summary.histogram(\n",
    "#                     name=var.op.name + '/gradients', \n",
    "#                     values=grad)\n",
    "        \n",
    "#         train_op = optimizer.minimize(\n",
    "#             loss=total_loss,\n",
    "#             global_step=tf.train.get_global_step())\n",
    "        \n",
    "#         # Add evaluation metrics for TRAIN mode.\n",
    "#         accuracy_train = tf.metrics.accuracy(\n",
    "#             labels=labels, \n",
    "#             predictions=predictions[\"classes\"])\n",
    "\n",
    "#         # Add summary operation for training accuracy visualizaiton.\n",
    "#         tf.summary.scalar(\n",
    "#             name='accuracy_train',\n",
    "#             tensor=accuracy_train[0])\n",
    "        \n",
    "#         train_summary_hook = tf.train.SummarySaverHook(\n",
    "#             save_steps=10,\n",
    "#             output_dir='models/q2-train',\n",
    "#             summary_op=tf.summary.merge_all())\n",
    "        \n",
    "#         return tf.estimator.EstimatorSpec(\n",
    "#             mode=mode,\n",
    "#             loss=total_loss, \n",
    "#             train_op=train_op,\n",
    "#             training_hooks=[train_summary_hook])\n",
    "    \n",
    "    \n",
    "#     # Add evaluation metrics for EVAL mode.\n",
    "#     accuracy_valid = tf.metrics.accuracy(\n",
    "#         labels=labels, \n",
    "#         predictions=predictions[\"classes\"])\n",
    "    \n",
    "#     # Add summary operation for validation accuracy visualizaiton.\n",
    "#     tf.summary.scalar(\n",
    "#         name='accuracy_validation',\n",
    "#         tensor=accuracy_valid[0])\n",
    "    \n",
    "#     eval_metric_ops = {\"accuracy\": accuracy_valid}\n",
    "    \n",
    "#     eval_summary_hook = tf.train.SummarySaverHook(\n",
    "#         save_steps=1,\n",
    "#         output_dir='models/q2-train',\n",
    "#         summary_op=tf.summary.merge_all())\n",
    "    \n",
    "#     return tf.estimator.EstimatorSpec(\n",
    "#         mode=mode, \n",
    "#         loss=total_loss, \n",
    "#         eval_metric_ops=eval_metric_ops,\n",
    "#         training_hooks=[eval_summary_hook])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
