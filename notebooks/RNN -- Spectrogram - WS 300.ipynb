{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Access data\n",
    "\n",
    "def get_data(data_dir, labels_path):\n",
    "    data_filenames = glob.glob(os.path.join(data_dir, '*npy'))\n",
    "    random.shuffle(data_filenames)\n",
    "\n",
    "    num_examples = len(data_filenames)\n",
    "\n",
    "    max_length = 0\n",
    "    for i, df in enumerate(data_filenames):\n",
    "        max_length = max(max_length, np.load(df).shape[0])\n",
    "    \n",
    "    labels_dict = {}\n",
    "    get_labels_dict(labels_path, labels_dict)\n",
    "    \n",
    "    X = np.zeros([num_examples, max_length])\n",
    "    Y = np.zeros(num_examples)\n",
    "    for i, df in enumerate(data_filenames):\n",
    "        data = np.load(df)\n",
    "        X[i, :data.shape[0]] = data\n",
    "        \n",
    "        label_key = df.split('/')[-1].split('.')[0].split('_')[0]\n",
    "        Y[i] = labels_dict[label_key]\n",
    "\n",
    "    # Convert -1 labels to 0\n",
    "    Y[np.where(Y == -1)] = 0\n",
    "        \n",
    "    return X.astype(np.float32), Y.astype(np.float32)\n",
    "\n",
    "\n",
    "def get_random_data(data_dir, labels_path, num_examples):\n",
    "    data_filenames = glob.glob(os.path.join(data_dir, '*npy'))\n",
    "    random.shuffle(data_filenames)\n",
    "\n",
    "    max_length = 0\n",
    "    for i in range(num_examples):\n",
    "        df = data_filenames[i]\n",
    "        max_length = max(max_length, np.load(df).shape[0])\n",
    "    \n",
    "    labels_dict = {}\n",
    "    get_labels_dict(labels_path, labels_dict)\n",
    "    \n",
    "    X = np.zeros([num_examples, max_length])\n",
    "    Y = np.zeros(num_examples)\n",
    "    for i in range(num_examples):\n",
    "        df = data_filenames[i]\n",
    "        \n",
    "        data = np.load(df)\n",
    "        X[i, :data.shape[0]] = data\n",
    "        \n",
    "        label_key = df.split('/')[-1].split('.')[0].split('_')[0]\n",
    "        Y[i] = labels_dict[label_key]\n",
    "\n",
    "    # Convert -1 labels to 0\n",
    "    Y[np.where(Y == -1)] = 0\n",
    "        \n",
    "    return X.astype(np.float32), Y.astype(np.float32)\n",
    "\n",
    "\n",
    "def get_random_spect_data(data_dir, labels_path, num_examples=None):\n",
    "    data_filenames = glob.glob(os.path.join(data_dir, '*npy'))\n",
    "    random.shuffle(data_filenames)\n",
    "\n",
    "    max_width = 0\n",
    "    max_length = 0\n",
    "    if num_examples is None:\n",
    "        for i, df in enumerate(data_filenames):\n",
    "            max_width = max(max_width, np.load(df).shape[0])\n",
    "            max_length = max(max_length, np.load(df).shape[1])\n",
    "        num_examples = i + 1\n",
    "    else:\n",
    "        for i in range(num_examples):\n",
    "            df = data_filenames[i]\n",
    "            max_width = max(max_width, np.load(df).shape[0])\n",
    "            max_length = max(max_length, np.load(df).shape[1])\n",
    "    \n",
    "    labels_dict = {}\n",
    "    get_labels_dict(labels_path, labels_dict)\n",
    "    \n",
    "    X = np.zeros([num_examples, max_length, max_width])\n",
    "    Y = np.zeros(num_examples)\n",
    "    for i in range(num_examples):\n",
    "        df = data_filenames[i]\n",
    "        \n",
    "        data = np.load(df)\n",
    "        X[i, :data.shape[1], :data.shape[0]] = data.T\n",
    "        \n",
    "        label_key = df.split('/')[-1].split('.')[0].split('_')[0]\n",
    "        Y[i] = labels_dict[label_key]\n",
    "\n",
    "    # Convert -1 labels to 0\n",
    "    Y[np.where(Y == -1)] = 0\n",
    "        \n",
    "    return X.astype(np.float32), Y.astype(np.float32)\n",
    "\n",
    "\n",
    "def get_labels_dict(reference_path, reference):\n",
    "    with open(reference_path) as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "        for row in spamreader:\n",
    "            reference[row[0]] = row[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Methods to account for variable sequence lengths (call for each batch)\n",
    "def length(sequence):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), 2))\n",
    "    length = tf.reduce_sum(used, 1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length\n",
    "\n",
    "def last_relevant(output, length):\n",
    "    batch_size = tf.shape(output)[0]\n",
    "    max_length = tf.shape(output)[1]\n",
    "    out_size = int(output.get_shape()[2])\n",
    "    index = tf.range(0, batch_size) * max_length + (length - 1)\n",
    "    flat = tf.reshape(output, [-1, out_size])\n",
    "    relevant = tf.gather(flat, index)\n",
    "    \n",
    "    return relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def _variable_on_cpu(name, shape, initializer):\n",
    "    \"\"\"\n",
    "    Helper to create a Variable stored on CPU memory.\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "        var = tf.get_variable(\n",
    "            name=name, \n",
    "            shape=shape, \n",
    "            dtype=tf.float32, \n",
    "            initializer=initializer)\n",
    "\n",
    "    return var\n",
    "\n",
    "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "    \"\"\"\n",
    "    Helper to create an initialized Variable with weight decay.\n",
    "    \"\"\"\n",
    "\n",
    "    var = _variable_on_cpu(\n",
    "        name=name,\n",
    "        shape=shape,\n",
    "        initializer=tf.truncated_normal_initializer(\n",
    "            stddev=stddev,\n",
    "            dtype=tf.float32))\n",
    "\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, \n",
    "                                   name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)\n",
    "\n",
    "    return var\n",
    "\n",
    "\n",
    "def loss(unscale_logits, labels):\n",
    "    \"\"\"\n",
    "    Add L2Loss to all the trainable variables.\n",
    "    \"\"\"\n",
    "\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=labels,\n",
    "        logits=unscale_logits,\n",
    "        name='cross_entropy')\n",
    "\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "\n",
    "    tf.add_to_collection('losses', cross_entropy_mean)\n",
    "\n",
    "    total_loss = tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn(features):\n",
    "    \n",
    "    features = features['x']\n",
    "    \n",
    "    # Input Layer\n",
    "    input_layer = w\n",
    "\n",
    "    # RNN layer #1\n",
    "    with tf.variable_scope(name_or_scope='rnn') as scope:\n",
    "        n_units = 500\n",
    "#         base_cell = tf.nn.rnn_cell.BasicRNNCell(n_units, activation=tf.nn.relu) tf.nn.rnn_cell.BasicLSTMCell\n",
    "#         rnn_cell = tf.contrib.rnn.MultiRNNCell([tf.nn.rnn_cell.BasicRNNCell(n_units, activation=tf.nn.relu) for _ in range(2)])\n",
    "#         rnn_cell = tf.contrib.rnn.MultiRNNCell([tf.nn.rnn_cell.BasicLSTMCell(n_units) for _ in range(2)])\n",
    "\n",
    "#         outputs, rnn_state =  tf.nn.dynamic_rnn(rnn_cell, \n",
    "#                                            inputs=input_layer,\n",
    "#                                            # initial_state=initial_state,\n",
    "#                                            dtype=tf.float32,\n",
    "#                                            # sequence_length=length(input_layer),\n",
    "#                                            time_major=False,\n",
    "#                                            scope=scope.name)\n",
    "        \n",
    "        rnn_cell_fw = tf.contrib.rnn.MultiRNNCell([tf.nn.rnn_cell.BasicLSTMCell(n_units) for _ in range(2)])\n",
    "        rnn_cell_bw = tf.contrib.rnn.MultiRNNCell([tf.nn.rnn_cell.BasicLSTMCell(n_units) for _ in range(2)])\n",
    "\n",
    "        outputs, rnn_state =  tf.nn.bidirectional_dynamic_rnn(cell_fw=rnn_cell_fw,\n",
    "                                                              cell_bw=rnn_cell_bw,\n",
    "                                                              inputs=input_layer,\n",
    "                                                              dtype=tf.float32,\n",
    "                                                              time_major=False,\n",
    "                                                              scope=scope.name)\n",
    "        \n",
    "        outputs = tf.concat(outputs, 2)\n",
    "        \n",
    "        \n",
    "    # Fully connected layer #1\n",
    "    with tf.variable_scope(name_or_scope='fc1') as scope:\n",
    "\n",
    "        weights = _variable_with_weight_decay(\n",
    "            name='weights',\n",
    "            shape=[2 * n_units, 250], # needs to match output dimension of RNN\n",
    "            stddev=0.04,\n",
    "            wd=0.004)\n",
    "\n",
    "        biases = _variable_on_cpu(\n",
    "            name='biases',\n",
    "            shape=[250], # needs to match output dimension of RNN\n",
    "            initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "        pre_activation = tf.add(tf.matmul(outputs[:, -1, :], weights), biases)\n",
    "\n",
    "        fc1 = tf.nn.relu(\n",
    "            features=pre_activation,\n",
    "            name=scope.name)\n",
    "        \n",
    "    # Fully connected layer #2\n",
    "    with tf.variable_scope(name_or_scope='fc2') as scope:\n",
    "\n",
    "        weights = _variable_with_weight_decay(\n",
    "            name='weights',\n",
    "            shape=[250, 2], # needs to match output dimension of RNN\n",
    "            stddev=0.04,\n",
    "            wd=0.004)\n",
    "\n",
    "        biases = _variable_on_cpu(\n",
    "            name='biases',\n",
    "            shape=[2], # needs to match output dimension of RNN\n",
    "            initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "        pre_activation = tf.add(tf.matmul(fc1, weights), biases)\n",
    "\n",
    "        fc2 = tf.nn.relu(\n",
    "            features=pre_activation,\n",
    "            name=scope.name)\n",
    "\n",
    "    return fc2\n",
    "\n",
    "# def train_neural_network(): \n",
    "#     prediction = recurrent_neural_network(x_placeholder)\n",
    "#     cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=tf.reshape(y_placeholder, [batch_size, n_classes])))\n",
    "#     optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "#     epoch_batch_itr = 10\n",
    "    \n",
    "#     correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_placeholder, 1))\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correct, 'float32'))\n",
    "        \n",
    "#     with tf.Session() as sess:\n",
    "#         tf.global_variables_initializer().run()\n",
    "\n",
    "#         for epoch in range(hm_epochs):\n",
    "#             epoch_loss = 0\n",
    "#             for b in range(epoch_batch_itr):\n",
    "#                 batchX, batchY = generate_batch(train_path)\n",
    "#                 _, c = sess.run([optimizer, cost], feed_dict={x_placeholder: batchX, y_placeholder: batchY})\n",
    "#                 epoch_loss += c\n",
    "#             validation_batchX, validation_batchY = generate_batch(validation_path) \n",
    "#             print('Epoch', epoch, 'loss:', epoch_loss, 'Validation Accuracy:', accuracy.eval({x_placeholder: validation_batchX, y_placeholder: validation_batchY}))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def rnn_model_fn(features, labels, mode, params):\n",
    "\n",
    "    \"\"\"\n",
    "    Build model.\n",
    "    \"\"\"\n",
    "    model = params['model']\n",
    "#     unscale_logits = rnn(features)\n",
    "    unscale_logits = model(features)\n",
    "    \n",
    "    # Generate predictions for PREDICT and EVAL modes.\n",
    "    predictions = {\n",
    "        'classes': tf.argmax(input=unscale_logits, axis=1),\n",
    "        'probabilities': tf.nn.softmax(unscale_logits, name='softmax_tensor')\n",
    "    }\n",
    "    \n",
    "    ####################\n",
    "    # PREDICT\n",
    "    ####################\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT: \n",
    "        return tf.estimator.EstimatorSpec(mode=mode, \n",
    "                                          predictions=predictions['probabilities'])\n",
    "    \n",
    "    else:\n",
    "        # Calculate loss for both TRAIN and EVAL modes.\n",
    "        labels = tf.cast(labels, tf.int64)\n",
    "        total_loss = loss(unscale_logits, labels) #REFORM THIS LOSS FUNCTION\n",
    "\n",
    "        # Add summary operation for total loss visualizaiton.\n",
    "        tf.summary.scalar(\n",
    "            name='total_loss',\n",
    "            tensor=total_loss)\n",
    "        \n",
    "        \n",
    "        ####################\n",
    "        # TRAIN\n",
    "        ####################\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            \n",
    "            # Compute gradients using Gradient Descent Optimizer.\n",
    "            optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "            grads_vars = optimizer.compute_gradients(loss=total_loss)\n",
    "\n",
    "            # Add summary operations for gradient visualizations.\n",
    "            for grad, var in grads_vars:\n",
    "                if grad is not None:\n",
    "                    tf.summary.histogram(\n",
    "                        name=var.op.name + '/gradients', \n",
    "                        values=grad)\n",
    "\n",
    "            train_op = optimizer.minimize(\n",
    "                loss=total_loss,\n",
    "                global_step=tf.train.get_global_step())\n",
    "\n",
    "            # Add evaluation metrics for TRAIN mode.\n",
    "            accuracy_train = tf.metrics.accuracy(\n",
    "                labels=labels, \n",
    "                predictions=predictions[\"classes\"])\n",
    "\n",
    "            # Add summary operation for training accuracy visualizaiton.\n",
    "            tf.summary.scalar(\n",
    "                name='accuracy_train',\n",
    "                tensor=accuracy_train[0])\n",
    "\n",
    "            train_summary_hook = tf.train.SummarySaverHook(\n",
    "                save_steps=10,\n",
    "                output_dir='models/rnn',\n",
    "                summary_op=tf.summary.merge_all())\n",
    "\n",
    "            return tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=total_loss, \n",
    "                train_op=train_op,\n",
    "                training_hooks=[train_summary_hook])\n",
    "        \n",
    "        \n",
    "        ####################\n",
    "        # EVALUATE\n",
    "        ####################\n",
    "        else:\n",
    "            accuracy_valid = tf.metrics.accuracy(\n",
    "                labels=labels, \n",
    "                predictions=predictions[\"classes\"])\n",
    "\n",
    "            # Add summary operation for validation accuracy visualizaiton.\n",
    "            tf.summary.scalar(\n",
    "                name='accuracy_validation',\n",
    "                tensor=accuracy_valid[0])\n",
    "\n",
    "            eval_metric_ops = {\"accuracy\": accuracy_valid}\n",
    "\n",
    "            eval_summary_hook = tf.train.SummarySaverHook(\n",
    "                save_steps=1,\n",
    "                output_dir='models/rnn',\n",
    "                summary_op=tf.summary.merge_all())\n",
    "\n",
    "            return tf.estimator.EstimatorSpec(\n",
    "                mode=mode, \n",
    "                loss=total_loss, \n",
    "                eval_metric_ops=eval_metric_ops,\n",
    "                training_hooks=[eval_summary_hook])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function for building, training and evaluating model.\n",
    "def main(train_data, train_labels, eval_data, eval_labels, test_data):\n",
    "    \n",
    "    estimator_dir = 'models/rnn'\n",
    "    \n",
    "    # Delete directory containing events logs and checkpoints if it exists.\n",
    "    if tf.gfile.Exists(estimator_dir):\n",
    "        tf.gfile.DeleteRecursively(estimator_dir)\n",
    "\n",
    "    # Create directory containing events logs and checkpoints.\n",
    "    tf.gfile.MakeDirs(estimator_dir)\n",
    "    \n",
    "    # Create the Estimator.\n",
    "    classifier = tf.estimator.Estimator(\n",
    "        model_fn=rnn_model_fn, \n",
    "        model_dir=estimator_dir,\n",
    "        params={'model': rnn})\n",
    "\n",
    "#     for _ in range(100):\n",
    "    for _ in range(2):\n",
    "        \n",
    "        # Train the model.\n",
    "        train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={'x': train_data},\n",
    "            y=train_labels,\n",
    "#             batch_size=128,\n",
    "            batch_size=1,\n",
    "            num_epochs=None,\n",
    "            shuffle=True)\n",
    "\n",
    "        classifier.train(\n",
    "            input_fn=train_input_fn,\n",
    "#             steps=50)\n",
    "            steps=2)\n",
    "\n",
    "        # Evaluate the model and print results.\n",
    "        eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"x\": eval_data},\n",
    "            y=eval_labels,\n",
    "#             batch_size=,\n",
    "            num_epochs=1,\n",
    "            shuffle=False)\n",
    "    \n",
    "        eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "        print(eval_results)\n",
    "        \n",
    "    # Generate predictions on test set.\n",
    "    predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": test_data},\n",
    "      num_epochs=1,\n",
    "      shuffle=False)\n",
    "\n",
    "    predictions = np.array(list(classifier.predict(input_fn=predict_input_fn))).T\n",
    "    \n",
    "    return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_dir = 'data/sequence'\n",
    "\n",
    "# X_train, Y_train = get_data(os.path.join(data_dir, 'training/sub'), \n",
    "#                             os.path.join(data_dir, 'REFERENCE.csv'))\n",
    "                            \n",
    "# X_valid, Y_valid = get_data(os.path.join(data_dir, 'validation/sub'), \n",
    "#                             os.path.join(data_dir, 'REFERENCE.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "\n",
    "data_dir = '/Volumes/light/deeplearning_proj/data/spectrogram/training/sub_ws300'\n",
    "labels_path = '/Volumes/light/deeplearning_proj/data/spectrogram/training/REFERENCE.csv'\n",
    "num_examples = 1000\n",
    "\n",
    "X_train, Y_train = get_random_spect_data(data_dir, labels_path, num_examples)\n",
    "\n",
    "# Load validation data\n",
    "data_dir = '/Volumes/light/deeplearning_proj/data/spectrogram/validation/sub_ws300'\n",
    "labels_path = '/Volumes/light/deeplearning_proj/data/spectrogram/validation/REFERENCE.csv'\n",
    "\n",
    "X_valid, Y_valid = get_random_spect_data(data_dir, labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'models/rnn', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into models/rnn/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.3314, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 2 into models/rnn/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.19251.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-10-00:21:19\n",
      "INFO:tensorflow:Restoring parameters from models/rnn/model.ckpt-2\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-10-00:21:41\n",
      "INFO:tensorflow:Saving dict for global step 2: accuracy = 0.489399, global_step = 2, loss = 1.37614\n",
      "{'accuracy': 0.48939928, 'loss': 1.3761444, 'global_step': 2}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models/rnn/model.ckpt-2\n",
      "INFO:tensorflow:Saving checkpoints for 3 into models/rnn/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.964078, step = 3\n",
      "INFO:tensorflow:Saving checkpoints for 4 into models/rnn/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.764376.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-10-00:21:57\n",
      "INFO:tensorflow:Restoring parameters from models/rnn/model.ckpt-4\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-10-00:22:19\n",
      "INFO:tensorflow:Saving dict for global step 4: accuracy = 0.490283, global_step = 4, loss = 1.93215\n",
      "{'accuracy': 0.49028268, 'loss': 1.9321529, 'global_step': 4}\n",
      "INFO:tensorflow:Restoring parameters from models/rnn/model.ckpt-4\n"
     ]
    }
   ],
   "source": [
    "pred = main(X_train, Y_train, X_valid, Y_valid, X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu = [0.6042403,0.65459365,0.66431093,0.63427562,0.61749119,0.63074207,0.61042404,0.62720847,0.6448763,0.63515902,0.61925793,0.64045936,0.62102473,0.62544167,0.62632507,0.66342759,0.62014133,0.62544167,0.6033569,0.61484098,0.63515902,0.62014133,0.63604242,0.6492933,0.5998233,0.62809187,0.61484098,0.6448763,0.63692582,0.63692582,0.64045936,0.63692582,0.64045936,0.64134276,0.63869256,0.63869256,0.64222616,0.63957596,0.64222616,0.61307418,0.62455833,0.6051237,0.63162541,0.6042403,0.6475265,0.62897527,0.63427562,0.64134276,0.6475265,0.63604242,0.63427562,0.62897527,0.62897527,0.63250881,0.63074207,0.62720847,0.63957596,0.63515902,0.63780922,0.64310956,0.62367493,0.6024735,0.59363955,0.5998233,0.59363955,0.66342759,0.62632507,0.64045936,0.63339221,0.63515902,0.64134276,0.64134276,0.63957596,0.63692582,0.63780922,0.61307418,0.62014133,0.58745581,0.61484098,0.62190813,0.65194345,0.6448763,0.63957596,0.64222616,0.6484099,0.6492933,0.64310956,0.64134276,0.63957596,0.6466431,0.63604242,0.63427562,0.63957596,0.63604242,0.59363955,0.63692582,0.62632507,0.62897527,0.63339221,0.63515902]\n",
    "plt.plot(accu)\n",
    "plt.ylim(0., 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
